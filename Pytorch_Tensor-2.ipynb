{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxN1F2sR1lbgywqE3l2cd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjdgoruds2/AI-_assignment/blob/master/Pytorch_Tensor-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rq4obvdZ2id",
        "outputId": "282ec109-e071-404b-e9ab-d7e05aedb56e"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "x=Variable(torch.tensor([[2.]]),requires_grad=True)\n",
        "print('x=',x)\n",
        "print('x.data=',x.data)\n",
        "print('x.grad=',x.grad)\n",
        "print('x.grad_fn()=',x.grad_fn)\n",
        "\n",
        "y=x*x*3\n",
        "print('\\ny=',y)\n",
        "print('y.data=',y.data)\n",
        "print('y.grad=',y.grad)\n",
        "print('y.grad_fn()=',y.grad_fn)\n",
        "z=y**2\n",
        "print('\\nz=',z)\n",
        "print('z.data=',z.data)\n",
        "print('z.grad=',z.grad)\n",
        "z.backward()\n",
        "print('\\nAfter invocation of backward()')\n",
        "print('\\nx=',x)\n",
        "print('x.data=',x.data)\n",
        "print('x.grad=',x.grad)\n",
        "print('x.grad_fn()=',x.grad_fn)\n",
        "print('\\ny=',y)\n",
        "print('y.data=',y.data)\n",
        "print('y.grad=',y.grad)\n",
        "print('y.grad_fn()=',y.grad_fn)\n",
        "print('\\nz=',z)\n",
        "print('z.data=',z.data)\n",
        "print('z.grad=',z.grad)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x= tensor([[2.]], requires_grad=True)\n",
            "x.data= tensor([[2.]])\n",
            "x.grad= None\n",
            "x.grad_fn()= None\n",
            "\n",
            "y= tensor([[12.]], grad_fn=<MulBackward0>)\n",
            "y.data= tensor([[12.]])\n",
            "y.grad= None\n",
            "y.grad_fn()= <MulBackward0 object at 0x7f170c911860>\n",
            "\n",
            "z= tensor([[144.]], grad_fn=<PowBackward0>)\n",
            "z.data= tensor([[144.]])\n",
            "z.grad= None\n",
            "\n",
            "After invocation of backward()\n",
            "\n",
            "x= tensor([[2.]], requires_grad=True)\n",
            "x.data= tensor([[2.]])\n",
            "x.grad= tensor([[288.]])\n",
            "x.grad_fn()= None\n",
            "\n",
            "y= tensor([[12.]], grad_fn=<MulBackward0>)\n",
            "y.data= tensor([[12.]])\n",
            "y.grad= None\n",
            "y.grad_fn()= <MulBackward0 object at 0x7f170c911a58>\n",
            "\n",
            "z= tensor([[144.]], grad_fn=<PowBackward0>)\n",
            "z.data= tensor([[144.]])\n",
            "z.grad= None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}